---
title: "SARFRAZ_FINAL"
author: "Hussain Sarfraz"
date: "12/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(dplyr)
library(scales)
require(weights)
require(anesrake)
```

# Part A

## Question 1

### Part A

To answer this question I used the **setwd()** function to establish my working directory. Doing this allows me to load the datasets that I want. After I did this step I used the **read_csv()** function to load in the Alabama and Alaska datasets together. 

* The **alabama.pop** object stores in the population data for **Alabama**
* The **alaska.pop** object stores in the population data for **Alaska**

```{r}
setwd("C:/Users/hussainsarfraz/Desktop/DATA 210")
alabama.pop <-  read_csv('sub-est2016_1.csv')
alaska.pop <-  read_csv('sub-est2016_2.csv')
```

After I loaded in the Alabama and Alaska population data I needed to append the two datasets together. Before I append the datasets I need to figure out what append function I need to use. 

1. To determine this I first compare the number of columns in the **alabama.pop** and **alaska.pop** datasets. The result that I get is **TRUE** which means that both datasets have a equal number of columns.
      + I did this using the **ncol()** function which gives me the number of columns in each dataset
2. The next step is to see the **order of columns** in both datasets and to see if the column order for both datasets are the same. The result that I got were all **TRUE** which means that the order of columns in both datasets are the same.
      + I did this using the **colnames()** function which lists down all the column names for each dataset in the order that they come in
3. After determining that the two datasets had a **equal amount of columns** and the **same order of columns** I decided to use the **rbind()** function to append the two datasets together. 
      + The **rbind()** function is used to append datasets when the **number** (and **order**) of columns are the same in both datasets.
      + If the columns number and order for both datasets were different then I would need to use the **rbind.fill()** (from **plyr package**) or **bind_rows()** (from **dplyr package**) functions to append the datasets

```{r}
#checking what append function to use
ncol(alabama.pop) == ncol(alaska.pop) #checking column number
colnames(alabama.pop) == colnames(alaska.pop) #checking proper order
#using rbind() append function because number of columns and order is the same
alabama.alaska.pop <- rbind(alabama.pop,alaska.pop)
```

### Part B

I created a object called **state.pop** which stores in the dataset of the population information for each state. I used the **read_csv()** function to do this.

I then used the **unique()** function to see all the state names in the **state.pop** dataset. There are a total of **51 states** under the **STNAME** column. When looking at the state names I realized that the **District of Columbia** is not a state, but a district. But when I searched online about this, some sources said that many lists include the **District of Columbia** in the U.S states list (**Puerto Rico** is also included in some lists which make 52 states but in this dataset **Puerto Rico** was not under the **STNAME** column.

```{r}
state.pop <-  read_csv('sub-est2016_all.csv')

unique(state.pop$STNAME) #51 names under 'STNAME' column in dataset - District of Columbia not a state but district

```

### Part C

For this problem I created a object called **state.pop.tidy** which would store in any new changes that I make to the **state.pop** dataset. The reason I did this was to keep a original copy of the population information for each state in case I would want to compare the new dataset with the old one.

I used the **subset()** function to only display the columns **NAME, STNAME, POPESTIMATE2012**. I used the **select()** function to pick the specific column names.

```{r}
state.pop.tidy <- subset(state.pop, select = c(NAME, STNAME, POPESTIMATE2012))
```

### Part D

The **state.pop.tidy** dataset includes the population total for each state as well as the population stats for each city and town. My goal is to filter the dataset to only include the popultation totals for each state. 

1. To do this I used the **subset()** function to only include the variables in the **NAME** and **STNAME** columns that have a common match. I did this because I noticed that the **NAME** column repeated the state names and the **STNAME** column repeated the state name once and then the state's city and town. Using **NAME == STNAME** in the select function helped me filter out the rows that had the population data for each city and town and just gave the rows that have the total state population data. 
2. I then created an object called **state.pop.tidy.51** which stored in the new dataset that only has each states population totals. 

```{r}
state.pop.tidy.51 <- subset(state.pop.tidy, NAME == STNAME) #filtering rows
```

My next task was to make sure there are no repeating **observations** or **columns** in the **state.pop.tidy.51** dataset. 

3. I first wanted to focus on which **rows** had repeating data. To do this I used the **table()** function to count the number of times each state appears in the **NAME** column. I then used the **sort()** function to see which states where repeated the most.
      + When I ran the code my output was the **District of Columbia** which was repeated two times in the **state.pop.tidy.51** dataset. This means that I have to remove one of those rows from the dataset.
4. To remove the rows that had the **District of Columbia** repeated I first needed to figure out which row number I needed to remove from my dataset. I used the **which()** function to figure this out and the result was that rows **9** and **10** in the **state.pop.tidy.51** dataset had the **District of Columbia** repeated

```{r}
sort(table(state.pop.tidy.51$NAME), decreasing = T) #seeing which state repeats

which(state.pop.tidy.51$NAME == "District of Columbia") #finding which row number contain the repeating values
```

5. I then needed to check if rows **9** and **10** were exactly the same and did not have different information. The output that I got was that both rows were exactly the same which meant that I could remove one of the rows. I decided to remove row **9**
6. I then removed **row 9** by using bracket notation and updated this change on the **state.pop.tidy.51** dataset. Now the **state.pop.tidy.51** dataset has a total of **51 rows**

```{r}
state.pop.tidy.51[9, ] %in% state.pop.tidy.51[10, ] #checking if rows are same
#state.pop.tidy.51[9, ] == state.pop.tidy.51[10, ] #different way to check

state.pop.tidy.51 <- state.pop.tidy.51[-9, ] #making dataset have a total of 51 rows
```

7. Next, I had to make sure that I remove any **repeating columns** and I noticed that **NAME** and **STNAME** have the exact same values. I also double checked this and then removed the **STNAME** column from the **state.pop.tidy.51** dataset

```{r}
state.pop.tidy.51$NAME %in% state.pop.tidy.51$STNAME #checking to see if columns have same values
#state.pop.tidy.51$NAME == state.pop.tidy.51$STNAME #different way to check

state.pop.tidy.51$STNAME <- NULL
```

### Part E

I used the **read.csv()** function to read in the dataset about the square mileage for each state. The object **state.areas** would store in this dataset.

```{r}
state.areas <- read.csv('state-areas.csv')
```

After loading in the **state.areas** dataset I needed to merge this dataset with the **state.pop.tidy.51** dataset. I decided to use a **inner-merge** to merge these datasets together since I did not want any un-matched rows to be displayed. The whole purpose for this merge was to bring in the **state square mileage** and **state population** into one dataset. If a **inner-merge** was not performed then there would be a **NA** value under the **POPESTIMATE2012** column for one of the states.

The **state.area.pop.merge** object stores in the merged dataset.

```{r}
#inner-merge
state.area.pop.merge <- merge(x = state.pop.tidy.51, 
      y = state.areas, 
      by.x = 'NAME',
      by.y = 'state') 
```

After I merged the **state.pop.tidy.51** and **state.areas** dataset I used the **unique()** function to display all the state names that were matched. I also wanted to see which observations were not matched which is why I also used the **anti_join()** function. The result that I got was that **Puerto Rico** was not a common state match for both datasets.

```{r}
#Which observations can be matched?
unique(state.area.pop.merge$NAME)

#which observations are not a common match
anti_join(x = state.areas, 
          y = state.pop.tidy.51, 
          by = c('state' =  'NAME')) #	Puerto Rico is not the common match
```

### Part F

I used the **mutate()** function to create a new column/variable that calculates the population density for each state. The column name that stores this information is called **2012 State Population Density**. I got the population density by dividing the **POPESTIMATE2012** variable with the **area..sq..mi.** variable. 

There is also another way to create the **2012 State Population Density** variable, but I put that in comments since I already used the **mutate()** function to accomplish the same task.

```{r}
state.area.pop.merge <- state.area.pop.merge %>%
  mutate('2012 State Population Density' = POPESTIMATE2012/area..sq..mi.)

#other way to do it
#state.area.pop.merge$`2012 State Population Density` <- state.area.pop.merge$POPESTIMATE2012/state.area.pop.merge$area..sq..mi.
```

### Part G

I loaded in the dataset which includes the economic data for each sector within each state. The **state.economic.data** object stores in this dataset.

I then used **bracket notation** to get rid of the first row. I used the **minus sign** to get rid of the first row.

**NOTE**: The object **state.economic.data.tidy** stores in any changes made to the **state.economic.data** dataset. I am doing this so I can always look back at the original dataset for reference.

```{r}
state.economic.data <- read.csv('ECN_2012_US_52A1.csv')

state.economic.data.tidy <- state.economic.data[-1, ] #removing 1st row
```

### Part H

Before I start to answer this question there were some data cleaning steps that I took. I used the **unique()** function to check if there was any data in the **GEO.annotation.id** and **NAICS.annotation.id** columns. The outputs I got when I ran both functions was a blank meaning there was no data in those columns. This is why I removed those two columns from the dataset.

```{r}
#checking if there are any values in the columns
unique(state.economic.data.tidy$GEO.annotation.id)
unique(state.economic.data.tidy$NAICS.annotation.id)

#removing two empty columns
state.economic.data.tidy$GEO.annotation.id <- NULL
state.economic.data.tidy$NAICS.annotation.id <- NULL
```

Now when I was reading the column descriptions in the **state.economic.data** dataset (the original, untouched dataset). The column that stood out to me was the **RCPTOT** column which had this description: **Revenue ($1000)**

When I read this description I realized that this was the column that I needed to focus on since it contained all the revenues earned for each state and sector. The question was asking **Find the total revenue per sector by state**. The values under the **RCPTOT** column contains the **total revenues** for each sector by state. 

Before I began displaying the total revenue I first had to remove the empty rows under the **RCPTOT** column. Because this dataset did not come with a code book or did not specify what was the formula to calculate the revenue I assumed that the blank observations under the **RCPTOT** column **can not be calculated and is missing data**. Since I had no way to calculate or find this data I decided to make all the blank observations (under the **RCPTOT** column) **NA** values and then remove all of them.

```{r}
#making empty rows NA
state.economic.data.tidy$RCPTOT[state.economic.data.tidy$RCPTOT==''] <- NA

#removing the NA rows
state.economic.data.tidy <- state.economic.data.tidy[complete.cases(state.economic.data.tidy$RCPTOT),]
```

After I removed all the **empty rows under the RCPTOT column** I realized that there were many rows that were repeating in the **state.economic.data.tidy**. This meant that I needed to go a step further and remove the repetitive rows because if I were to aggregate and add the revenue for the repetitive rows then I would not be getting the correct **total revenue per sector by state**. As mentioned before, the revenues under the **RCPTOT** column are the total revenues per sector by state. 

I am saying this because I noticed that the sectors under the **NAICS.display.label** column are repeated and there are no altercations to the sector name which led me to believe that the same sector names are repeated. Also, when I compared the 1st and 2nd rows in the **state.economic.data.tidy** dataset (both rows had the same sector name which is why I compared them). I noticed that both rows were entirely the same. The only difference in observations was the values under **NAICS.id** column. This strengthened my assumption that the revenues under the **RCPTOT** column are the **total revenue per sector by state**. I personally do not know why the ids under the **NAICS.id** column were different. I assume that this was an error in the data entry but as I said before I was not given much context about the dataset and what each column means so I just went with what I thought was best/should be done.

```{r}
#checking to see the row commonalities 
state.economic.data.tidy[1, ] == state.economic.data.tidy[2, ]
```

Now after I verified my assumption that I need to remove repeating rows in the **state.economic.data.tidy** dataset, the next step was to remove the repeating rows. I decided to use the **filter()** function to remove the repeating rows. Inside the **filter()** function I used the **!duplicated()** function to tell R to only give me the rows under the **NAICS.display.label** that are **not repeated/duplicated**. I also included the column **GEO.display.label** to make sure the that **repeated sector names under each individual state** are removed.

Now before I was going to eliminate the repeated rows I wanted to double-check and make sure the **filter()** function was successful in removing the repeated rows per sector by state. This is why I created the **row.check** object which stored in the number of unique combinations that can be made by the **GEO.display.label** and the **NAICS.display.label** columns. I chose these two columns because I wanted to eliminate the repeated sector names **under each individual state**

```{r}
#checking to see how many unique combinations can be made between the states and each sector
row.check <- nrow(unique(state.economic.data.tidy[ ,c("GEO.display.label", "NAICS.display.label")])) #output: 1272 rows

#filtering the non-duplicate rows combos of state rows 
state.economic.data.tidy <- state.economic.data.tidy %>%
  filter(!duplicated(state.economic.data.tidy[ ,c("GEO.display.label", "NAICS.display.label")])) 
```

After I filtered out the repeating sectors by state from the **state.economic.data.tidy** dataset I then compared the number of rows in the **state.economic.data.tidy** dataset with the **row.check** object to make sure I filtered the dataset correctly. The output was **TRUE** which means that I filtered out the correct number of rows from the dataset. 

I also used another way to double-check if I do not have any additional repeating rows by arranging the count of each sector from the **state.economic.data.tidy** dataset from highest to lowest. The result was that all my counts were ones which meant that there were no repeating sector names under the **state.economic.data.tidy** dataset. 

```{r}
#checking to see if I correctly filtered out the repetitive rows
nrow(state.economic.data.tidy) == row.check

#checking to see if filter worked and there are no repeats in sector labels
# state.economic.data.tidy %>%
#   group_by(GEO.display.label,NAICS.display.label) %>%
#   summarize(row.count = n()) %>%
#   arrange(desc(row.count))
```

### Part I

Before I merged the per sector by state economic dataset with the state population density dataset I removed the un-necessary columns in the **state.economic.data.tidy** dataset. I did this so the merged dataset can be easier to look at when I merge it. The columns that I kept were:

* GEO.display.label
* NAICS.display.label
* RCPTOT

I kept the **GEO.display.label** column because it was essential for the merge. The **NAICS.display.label** column was kept because it gave each sectors name. And the **RCPTOT** column was kept because it had the total revenue for each sector by state.

```{r}
#removing un-necessary columns
state.economic.data.tidy <- state.economic.data.tidy[ ,-c(1:2)]
state.economic.data.tidy <- state.economic.data.tidy[ ,-2]
state.economic.data.tidy <- state.economic.data.tidy[ ,-c(3:4)]
state.economic.data.tidy <- state.economic.data.tidy[ ,-c(4:8)]
```

I then performed a **inner-merge** to merge the **state.economic.data.tidy** and **state.area.pop.merge** datasets together. I performed a **inner-merge** since I did not want any un-matched rows to be displayed in my merged dataset. I objective when doing this merge is to see the state square mileage **and** population all in one dataset. If I used a different type of merge then I would not be getting the state square mileage and population for some states.

I also used the **anti_join()** function to see if there are any un-matched rows between the two datasets. The result that I got was that there were no un-matched rows between the two datasets.

```{r}
#inner-merge
state.econ.pop.merge <- merge(x = state.economic.data.tidy, 
                              y = state.area.pop.merge, 
                              by.x = 'GEO.display.label',
                              by.y = 'NAME') 

#Another way to merge
# merge(x = state.area.pop.merge, 
#       y = state.economic.data.tidy , 
#       by.x = 'NAME',
#       by.y = 'GEO.display.label') %>% view()

#to check any unmatched values
# anti_join(x = state.economic.data.tidy, 
#       y = state.area.pop.merge, 
#       by = c('GEO.display.label'= 'NAME')) #output: no rows
```

After I successfully merged the two datasets together I removed the **POPESTIMATE2012** and **area..sq..mi.** columns from the datasets. I did this because my main focus when looking at the merged dataset is to only look at the revenue per sector (by state) and the state population density.

```{r}
#removing un-necessary columns in merged dataset 
state.econ.pop.merge <- state.econ.pop.merge[ ,-c(4:5)]
```

### Part J

I have divided this problem into different parts to explain what I have done in each part of the process when making the graph.

> Introduction: What I did before graphing

When I looked at the merged dataset **state.econ.pop.merge** I noticed that the state population density values were repeated many times (this is the **`2012 State Population Density`** column). This is because in the **state.economic.data.tidy** dataset the state name was repeated many times because there were many sectors in each state. But in the **state.area.pop.merge** dataset the state names only appeared once because each state had one population density value. Because of the difference in how many times the state names appeared in each dataset, R decided to just repeat the population density amount for each state to fit the exact dimensions of the **state.economic.data.tidy** dataset. 

Because of this repetition I had to make sure I used the **group_by()** function before making my graph to make sure I do not display the revenue **per sector by state** (doing this would also make the graph confusing since there would be so many groups being displayed on the graph)

I then used the **summarize()** function to calculate the sum of each states revenue (the **total.state.revenue** column stores in this value). I also created a column **state.pop.density** which stored in the **`2012 State Population Density`** values in the dataset. When I looked at the dataset at this stage I noticed that the state population density values were repeated and this is because in the **summarize()** function I did not perform any calculations on the **`2012 State Population Density`** column and asked R to just re-display all the values as it is.

Now to eliminate the repeated observations in under the **state.pop.density** column I used the **distinct()** function to eliminate the repetitions. **.keep_all=TRUE** is used to tell R to also include the other columns once the **distinct()** function runs. To use the **distinct()** function I had to first launch the **dplyr library**. 

* In **part H** I used the **!duplicated()** function to eliminate repeated rows. I did not use the **!duplicated()** function this time because the values under the **state.pop.density** column were numeric and the **distinct()** function successfully removed repeated numerical values while the **!duplicated()** function couldn't.

Now I will talk about the individual things that I did when making the graphs. The first graph is my **official answer**. I just included the second graph to show what I came across/saw before I made any changes to the **State Population Density and Total State Revenue** graph.

**NOTE**: I displayed the state total revenue in the graph and not the revenue per sector by state. This is because the question asked me to: Plot the relationship between state population density and **the state’s total revenue**

> My Graph with some filters (My Final Graph)

When making my graph I decided to use the **filter()** function to zoom in further on the lower left-hand corner of the graph since I noticed that there are a lot of points there. When I instantly looked at the original graph I noticed two main outliers.

* One outlier was towards the lower right-hand corner of the graph and had a low total state revenue, but high state population density which was around **8750-10000*
* The second outlier was towards the upper left-hand corner and had a low state population density, but high total revenue which was above **600,000,000 * 1,000** (I am multiplying this number by 1,000 because in the original dataset in **part G** called **state.economic.data** described the **RCPTOT** as **Revenue $1,000**. This means that any value of the revenue needs to be multiplied by 1,000)

I then set the title and X-Y values in the graph and also launched the **scales** library and used the **scale_y_continuous()** function to convert the numbers in the Y-axis from scientific notation to their actual number format. I did this because it was easier for me to read the total state revenue numbers. 

```{r}
state.econ.pop.merge %>%
  group_by(GEO.display.label) %>%
  summarise(total.state.revenue = sum(as.numeric(RCPTOT)),
            state.pop.density = `2012 State Population Density`) %>%
  distinct(state.pop.density, .keep_all= TRUE) %>%
  filter(state.pop.density < 400,
         total.state.revenue < 100000000) %>%
  ggplot(mapping = aes(x=state.pop.density , y=total.state.revenue)) +
  geom_point() +
  xlab("State Population Density") +
  ylab("State Total Revenue ($1000)") +
  ggtitle("State Population Density and Total State Revenue") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        plot.title = element_text(size = 10)) +
  scale_y_continuous(labels = comma)
```

> The Original Graph 

Here is how the graph would look like if I did not use the **filter()** function. As you can see it is really hard to see all the points in the lower left-hand corner of the graph since a couple of outliers mess up the overall graph display. 

```{r}
state.econ.pop.merge %>%
  group_by(GEO.display.label) %>%
  summarise(total.state.revenue = sum(as.numeric(RCPTOT)),
            state.pop.density = `2012 State Population Density`) %>%
  distinct(state.pop.density, .keep_all= TRUE) %>%
  ggplot(mapping = aes(x=state.pop.density , y=total.state.revenue)) +
  geom_point() +
  xlab("State Population Density") +
  ylab("State Total Revenue ($1000)") +
  scale_x_continuous(breaks=c(0,1250,2500,3750,5000,6250,7500,8750)) +
  ggtitle("State Population Density and Total State Revenue") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        plot.title = element_text(size = 10)) +
   scale_y_continuous(labels = comma)
```

> Overall Conclusions

My overall conclusions after seeing the graph is that as the states population density **increases** then the total state revenue **increases as well**. This means that as certain states have more people the total state revenue would increase. I believe this is the case because in states where there are more people there would be more people doing more jobs and investing in the states economy. Whereas, in states where there are less people there would be less people investing in the states economy. It is important to note that there are some outlier cases that do not follow my observations (just as I pointed out before).

**NOTE**: I did not color-code the points on the graph because I realized that the space around the graph would be smaller since the state color key would be too big.

# Part B

I loaded in the datafile **nes.rda** using the **load()** function. When I did this R automatically stored this dataset into a object called **nes** in my **global environment**.

I also did some data tidying and used the **gsub()** function to replace the **_** values in the column names by a **.** 

I then stored the tidy dataset into a new object called **nes.tidy**

```{r}
load('nes.rda')
names(nes) <- gsub('_','.',names(nes))
nes.tidy <- nes
```

## Question 1

> Answer: 67.32312 % of survey respondents voted for Barack Obama in the 2008 election

For this question I looked at **pages 97 and 98** in the **‘nes2012_codebook.pdf** codebook that came along with the dataset. I did this so I can understand the **interest.voted2008** and **interest.whovote2008** columns further. 

To start off, I used the **attributes()** or **unique()** functions to see all the different values under the **interest.voted2008** and **interest.whovote2008** columns. Even though I did have the **‘nes2012_codebook.pdf** codebook that detailed the attributes and values for each column I thought it would be good practice to also double-check and make sure there weren't any values under the **interest.voted2008** and **interest.whovote2008** columns that were not included in the codebook.

* **NOTE**: The **attributes()** function did not work since the columns did not have any attributes assigned to them. This is why I used the **unique()** function to see the different values stored in each column. I have also wrote the output that I got as a comment.

After I verified that the codebook talked about all the unique values under the **interest.voted2008** and **interest.whovote2008** columns I then decided to replace these values (listed in the table below) with **NA** values. I did this because these values are not important to me since somebody did not respond to them which is why I converted them to **NA** values.

Values converted to "NA" in "interest.voted2008"|                                   Values converted to "NA" in "interest.whovote2008"
-------------             |-------------
-8 (Don't know)           | -1 (Inapplicable) 
-9 (Refused)              | -8 (Don't know)  
____                      | -9  (Refused)

```{r}
#attributes(nes$interest.voted2008) #columns have no attributes
#unique(nes$interest.voted2008) #output: [1]  1  2 -8 -9
#unique(nes$interest.whovote2008) #output: [1]  1 -1 -8  2  5 -9

nes.tidy$interest.voted2008[nes.tidy$interest.voted2008 %in% c(-8,-9)] <- NA
nes.tidy$interest.whovote2008[nes.tidy$interest.whovote2008 %in% c(-1,-8,-9)] <- NA
```

My final step was to calculate the percentage. To do this I had to first find the **total number of people who claimed to have voted in 2008 election**. I made this decision because the question included the words: **of those who claimed to have voted in the 2008 election**. This means that my total (that I would use as a denominator when dividing) would be the total number of people who claimed to have voted in 2008 election.

1. I found the total number of people who claimed to have voted in 2008 election using the **subset()** function and used the **nrow()** function to get the total number of people who claimed to have voted in 2008 election.
      + I wrote this in my code: **nrow(subset(nes.tidy, interest.voted2008 == 1))** and my output was: **1,371**
      + The number one under the **interest.voted2008** column represent the people who voted in the 2008 election

I then had to find the **number of survey respondents that voted for Barack Obama in 2008**. 

2. I did this by using the **subset()** function to filter out the the people who did vote for Barack Obama in 2008 and then used the **nrow()** function to count the number of survey respondents who did vote for Barack Obama
      + I wrote this in my code: **nrow(subset(nes.tidy, interest.whovote2008 == 1))** and my output was: **923**
      + The number one under the **interest.whovote2008** column represent the people who voted for Barack Obama in the 2008 election

```{r}
nrow(subset(nes.tidy, interest.whovote2008 == 1))/nrow(subset(nes.tidy, interest.voted2008 == 1))*100
```

## Question 2

For this question I looked at **page 1,111** in the **‘nes2012_codebook.pdf** codebook and I did this so I can understand the **ftgr_fedgov** column better.

I used the **subset()** function to filter out the **ftgr.fedgov** column to only include feeling thermometer scores between **0 and 100**. I could have also used the **filter()** function to do the same task as well. 

I then used the **arrange()** function to see if I successfully filtered out the unnecessary feeling thermometer values.

```{r}
nes.tidy <- subset(nes.tidy, 0 <= ftgr.fedgov & ftgr.fedgov <= 100 )

#another way to filter values
#filter(nes.tidy, 0 <= ftgr.fedgov & ftgr.fedgov <= 100)

#to check lowest values
# nes.tidy %>%
#   transmute(ftgr.fedgov=ftgr.fedgov) %>%
#   arrange(ftgr.fedgov) %>%
#   head(10)
# 
#to check highest values
# nes.tidy %>%
#   transmute(ftgr.fedgov=ftgr.fedgov) %>%
#   arrange(desc(ftgr.fedgov)) %>%
#   head(10)
```

## Question 3

> Answer

What I Calculated                                              | Result/Output
-------------                                                  | -------------
**Weighted Mean** of Federal Government Feeling Thermometer    | 47.97778
**Un-Weighted Mean** of Federal Government Feeling Thermometer | 52.48652

> Calculating the Weighted Mean

When calculating the weighted mean I need to know which column in the **nes.tidy** dataset represents the weights. When looking at **page 32 and 84** in the codebook I read tht the **weight_full** variable is what I was looking for since it represents the weights of all samples. This is the exact description of this variable from the codebook:

* **'weight_full' is intended for the analysis of the combined samples. If you want to include all available cases, use this weight**

After I figured out which column represents the weights I used the **weighted.mean()** function to figure out the weighted mean of the Federal Government Feeling Thermometer.

* I also double-checked and used the formula for weighted means to actually check if my weighted mean calculation was correct.

```{r}
#calculating weighted mean
weighted.mean(nes.tidy$ftgr.fedgov, w= nes.tidy$weight.full) #47.97778

#(nes.tidy$ftgr.fedgov * nes.tidy$weight.full)/sum(nes.tidy$weight.full) #calculting weighted mean to check if my value is correct

```

> Calcultating the Un-Weighted Mean

For this question I had to get the total number of feeling thermometers in the dataset and then use that as my denominator. I would then get the sum of all the feeling thermometer values under the **ftgr.fedgov** column and use that as my numerator. 

* I also used the **mean()** function to see if my calculations were correct and I got the same result that I previously got. 

```{r}
sum(nes.tidy$ftgr.fedgov)/nrow(nes.tidy) #52.48652

#mean(nes.tidy$ftgr.fedgov) #another way to calculate mean

#checking if my mean calculations are correct
#sum(nes.tidy$ftgr.fedgov)/nrow(nes.tidy) == mean(nes.tidy$ftgr.fedgov) #both means are equal
```

## Question 4

For this question I looked at **page 128** in the **‘nes2012_codebook.pdf** codebook and I did this so I can understand the **prevote.regpty** column better.

The first thing I did was use the **unique()** function to double-check if all the observations in the **prevote.regpty** column match all the observations in codebook. It turned out that the unique observations in the **prevote.regpty** column matched the observations listed in the codebook.

Once I verified that there were not any missing attributes that I was overlooking I decided to create a new column called **democrat.or.republican** and make it a copy of the **prevote.regpty** column.

* I have also commented another way I could have made the **democrat.or.republican** column

```{r}
#creating a new variable that is a copy of the "prevote.regpty" variable
nes.tidy <- nes.tidy %>%
  mutate(democrat.or.republican = prevote.regpty)

#nes.tidy$democrat.or.republican <- nes.tidy$prevote.regpty #another way to create a copy of the "prevote.regpty" variable
```

In the **democrat.or.republican** column I replaced these observations (listed in the table below) with **NA** values. I did this because these values are not important to me since somebody did not respond to them **and** the observations did not represent a politician candidate from the republican or democratic party which is why I converted them to **NA** values. Also, the question stated that: **All other political attributes affiliations or unknowns should be set to 'NA'**

Values converted to "NA" in "democrat.or.republican" |                                Values Kept in "democrat.or.republican"
-------------                       | -------------
4 (None or 'independent')           | 1 (Democratic party)
5 (Other (SPECIFY))                 | 2 (Republican party)
-1 (Inapplicable)                   | _____
-8 (Don't know)                     | _____               
-9 (Refused)                        | _____     

Once I converted all the unecassary values to **NA** I then replaced the **one values** to display **Democratic Party** and the **two values** to display **Republican Party**. All of these conversions were to the **democrat.or.republican** column.

```{r}
#replacing the observations in the "democrat.or.republican" to "NA"s and more meaningful words
nes.tidy$democrat.or.republican[nes.tidy$democrat.or.republican %in% c(4,5,-1,-8,-9)] <- NA
nes.tidy$democrat.or.republican[nes.tidy$democrat.or.republican == 1] <- 'Democratic Party'
nes.tidy$democrat.or.republican[nes.tidy$democrat.or.republican == 2] <- 'Republican Party'
```

## Question 5

> Answer

What I Calculated                                                | Result/Output
-------------                                                    | -------------
**Un-Weighted Mean** of Democratic Party                         | 27.3400317 %  
**Un-Weighted Mean** of Republican Party                         | 9.9418297 %
Difference of Democratic and Republican **Weighted Means**       | 17.3982  %
-----------------------------------------------------------------| ---------------------
**Weighted Mean** of Democratic Party                            | 18.8461468 %
**Weighted Mean** of Republican Party                            | 15.0307565 %
Difference of Democratic and Republican **Un-Weighted Means**    | 3.81539 %

The difference between the **un-weighted means** for the Democratic and Republican party suggest that in the dataset there were more survey respondents who favored the Democratic Party rather than the Republican party. 

Whereas, when I look at the **weighted means** difference of the Democratic and Republican party I see that for the overall population the difference between Democratic and Republican party supporters is not that much. 

However, in both differences there seems to be a higher support for the Democratic party but the weighted mean lets me know that when looking at the overall population the percentage of support for Democrats and Republicans is pretty close.

> Calculating the Weighted and Unweighted Mean difference

To calculate the **un-weighted mean** and **weighted mean** I had to check if there were any **NA** values under the **prevote.regpty** column. I wanted to check this because when using functions to calculate the mean the rows that have **NA** values are automatically removed from the dataset and I need to make sure I include all rows. I found out that there are not **NA** values under the **prevote.regpty** column.

After I found that out I used the **prop.table()** and **table()** function to get a table of all the means of the observations under the **prevote.regpty** column. I was looking for the values under **one and two** since those values represented Democrats and Republicans. I stored this table in a object called **unweighted.means** and then used bracket notation to subtract the **un-weighted means** of the Democrats. 

* I applied the same logic when calculating the weighted means and the difference between Democrats and Republicans

```{r}
#checking to see if there are any "NA" values under the "prevote.regpty" column
#table(is.na(nes.tidy$prevote.regpty)) #all falses which means no "NA"s

#finding the Un-Weighted means for each value under the "prevote.regpty" column
unweighted.means <- prop.table(table(nes.tidy$prevote.regpty))*100
#finding the Un-Weighted mean difference 
unweighted.means[4] - unweighted.means[5] #17.3982 

#finding the Weighted means for each value under the "prevote.regpty" column
weighted.means <- wpct(nes.tidy$prevote.regpty, weight = nes.tidy$weight.full)*100
#finding the Weighted mean difference 
weighted.means[4] - weighted.means[5]
```

> Additional Section: Checking to see if my Un-Weighted Mean value for the Democratic Party was correct

**This section is optional to read and I am just exaplaining how I double-checked my work and calculated the weighted and un-weighted means through a different method**

I first started off by finding the un-weighted mean for the Democratic party respondents. To find the un-weighted mean I first had to count the number of Democratic respondents in the dataset and then divide that number by the total number of survey respondents.

I found the number of Democratic respondents by using the **subset()** function to first filter out the number of Democratic respondents in the dataset. I then used the **nrow()** function to count the number of Democratic respondents in the dataset.

* **NOTE:** I can also use the **filter()** function instead of the **subset()** function and would get the same result

I then found out the total number of respondents in the dataset by writing: **nrow(nes.tidy)**

What I calculated:                        | What value/result I got:
-------------                             | -------------
Total number of Democratic respondents    | 517
Total number of survey respondents        | 1891
Percentage of Democratic respondents      | 27.34003 %

When I divided the total number of **Democratic respondents** over the total number of **survey respondents** I multiplied that value by 100 since I was calculating a percentage. 

**NOTE**: When I was finding the average feeling thermometer score (part B-question 3) I did not need to multiply that number by 100 since I was calculating the average thermometer score. In this problem I finding the total amount of democratic respondents in the dataset over the total number of respondents. I want to see what **percentage** of the democratic respondents represent the whole dataset.

* This logic would also apply when I am calculating the number of **Republican respondents**

**NOTE**: I have also created a object called **democrat.mean.check.dataset** which essentially just gets the values in the **democrat.or.republican** column and converts the values that equal **Democratic Party** to one and all the other values to zero. This is used to indicate which respondents were in favor for the **Democratic Party** and which ones were not. 

* I used the same logic when creating the **republican.mean.check.dataset** object
* I am going to use the **democrat.mean.check.dataset** and **republican.mean.check.dataset** dataset to double-check if I calculated the **weighted mean** correctly. These datasets can be used to double-check my **un-weighted mean** and I have included that code in comments.

```{r}
nrow(subset(nes.tidy, democrat.or.republican=='Democratic Party')) #getting number of respondents that in favor for Democrats
#nrow(filter(nes.tidy, democrat.or.republican=='Democratic Party')) #another way to find democrat numbers

#calculating the un-weighted mean for democrat party
nrow(subset(nes.tidy, democrat.or.republican=='Democratic Party'))/nrow(nes.tidy)*100 #27.34003

#creating the "democrat.mean.check.dataset" to check my weighted mean calculations (can also be used to calculate the un-weighted means)
democrat.mean.check.dataset <- nes.tidy %>%
  mutate(demo.mean.count = ifelse((democrat.or.republican=='Democratic Party'), 1, 0),
            demo.mean.count = ifelse(is.na(demo.mean.count)==T, 0,demo.mean.count)
            )

#using "democrat.mean.check.dataset" to check my un-weighted mean calculation
# mean(democrat.mean.check.dataset$demo.mean.count)*100  #output: 27.34003

#checking if both un-weighted means are equal
# (mean(democrat.mean.check.dataset$demo.mean.count)*100) == (nrow(subset(nes.tidy, democrat.or.republican=='Democratic Party'))/nrow(nes.tidy)*100) #output: TRUE
```

> Checking to see if my Un-Weighted Mean value for the Republican Party was correct

Next, I was going to find the number/percentage of the Republican party respondents. 

I found the number of Republican respondents the same way that I found the number of Democratic respondents (I used the **subset()** and **nrow()** function.


What I calculated:                        | What value/result I got:
-------------                             | -------------
Total number of Republican respondents    | 188
Total number of survey respondents        | 1891
Percentage of Republican respondents      | 9.94183 %


```{r}
nrow(subset(nes.tidy, democrat.or.republican=='Republican Party')) #getting number of respondents that in favor for Republicans
#nrow(filter(nes.tidy, democrat.or.republican=='Republican Party')) #another way to find Republican numbers

#un-weighted mean for republican party
nrow(subset(nes.tidy, democrat.or.republican=='Republican Party'))/nrow(nes.tidy)*100 #9.94183

#creating the "republican.mean.check.dataset" to check my weighted mean calculations (can also be used to calculate the un-weighted means)
republican.mean.check.dataset <- nes.tidy %>%
  mutate(demo.mean.count = ifelse((democrat.or.republican=='Republican Party'), 1, 0),
            demo.mean.count = ifelse(is.na(demo.mean.count)==T, 0,demo.mean.count)
            )

#using "republican.mean.check.dataset" to check my un-weighted mean calculation
# mean(republican.mean.check.dataset$demo.mean.count)*100  #output: 9.94183

#checking if both un-weighted means are equal
# (mean(republican.mean.check.dataset$demo.mean.count)*100) == (nrow(subset(nes.tidy, democrat.or.republican=='Republican Party'))/nrow(nes.tidy)*100) #output: TRUE
```

> Finding the difference between the Democratic and Republican Un-Weighted Means

My final step was to calculate the difference in un-weighted means between the average number of Democrats and the Average number of Republicans. Finding this value was simple since I just needed to copy and paste the code that gave me the average result of the number of Democrats and Republicans. I then could subtract those two values which is how I got **17.3982 %** as the difference of un-weighted means between the **Democrat and Republican Party**

I also used **wpct()** function and **bracket notation** to double-check my weighted mean calculations. I used the  **democrat.mean.check.dataset** and **republican.mean.check.dataset** datasets to do this.

```{r}
#difference between democrat and rebublican un-weighted mean
(nrow(subset(nes.tidy, democrat.or.republican=='Democratic Party'))/nrow(nes.tidy)*100)-(nrow(subset(nes.tidy, democrat.or.republican=='Republican Party'))/nrow(nes.tidy)*100) 
#17.3982

# using the mean() functions only to check if I calculated the un-weighted mean correctly
# (mean(democrat.mean.check.dataset$demo.mean.count)*100)-(mean(republican.mean.check.dataset$demo.mean.count)*100)

# comparing the un-weighted mean values with what I previously got to see if the values are exactly the same
(mean(democrat.mean.check.dataset$demo.mean.count)*100)-(mean(republican.mean.check.dataset$demo.mean.count)*100)==(nrow(subset(nes.tidy, democrat.or.republican=='Democratic Party'))/nrow(nes.tidy)*100)-(nrow(subset(nes.tidy, democrat.or.republican=='Republican Party'))/nrow(nes.tidy)*100) 

# Double-checking the number I got for the weighted mean of Democratic voters
(wpct(democrat.mean.check.dataset$demo.mean.count, weight = democrat.mean.check.dataset$weight.full)*100)[2]

# Double-checking the number I got for the weighted mean of Republicans voters
(wpct(republican.mean.check.dataset$demo.mean.count, weight = republican.mean.check.dataset$weight.full)*100)[2]

# I am using the "democrat.mean.check.dataset" and "republican.mean.check.dataset" datasets to check if my weighted mean difference calculations are correct
(wpct(democrat.mean.check.dataset$demo.mean.count, weight = democrat.mean.check.dataset$weight.full)*100)[2]-(wpct(republican.mean.check.dataset$demo.mean.count, weight = republican.mean.check.dataset$weight.full)*100)[2]

# comparing the weighted mean values with what I previously got to see if the values are exactly the same
all.equal(((wpct(democrat.mean.check.dataset$demo.mean.count, weight = democrat.mean.check.dataset$weight.full)*100)[2]-(wpct(republican.mean.check.dataset$demo.mean.count, weight = republican.mean.check.dataset$weight.full)*100)[2]
),(weighted.means[4] - weighted.means[5]))
```
